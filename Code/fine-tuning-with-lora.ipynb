{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:11.554951Z",
     "iopub.status.busy": "2025-07-09T15:30:11.554414Z",
     "iopub.status.idle": "2025-07-09T15:30:11.558416Z",
     "shell.execute_reply": "2025-07-09T15:30:11.557773Z",
     "shell.execute_reply.started": "2025-07-09T15:30:11.554929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:12.296380Z",
     "iopub.status.busy": "2025-07-09T15:30:12.296097Z",
     "iopub.status.idle": "2025-07-09T15:30:18.176284Z",
     "shell.execute_reply": "2025-07-09T15:30:18.175564Z",
     "shell.execute_reply.started": "2025-07-09T15:30:12.296357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:20.975460Z",
     "iopub.status.busy": "2025-07-09T15:30:20.974498Z",
     "iopub.status.idle": "2025-07-09T15:30:44.894871Z",
     "shell.execute_reply": "2025-07-09T15:30:44.894280Z",
     "shell.execute_reply.started": "2025-07-09T15:30:20.975414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 15:30:27.790972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752075028.153001      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752075028.262295      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:20:36.170470Z",
     "iopub.status.busy": "2025-07-09T17:20:36.169924Z",
     "iopub.status.idle": "2025-07-09T17:20:36.176307Z",
     "shell.execute_reply": "2025-07-09T17:20:36.175789Z",
     "shell.execute_reply.started": "2025-07-09T17:20:36.170445Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:44.896519Z",
     "iopub.status.busy": "2025-07-09T15:30:44.895927Z",
     "iopub.status.idle": "2025-07-09T15:30:46.304957Z",
     "shell.execute_reply": "2025-07-09T15:30:46.304367Z",
     "shell.execute_reply.started": "2025-07-09T15:30:44.896496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66f2d7b85a84151b0499ea35b703b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7b93024e014b978bc41b73fc99ea7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02524990f7fc450eadb00683ee3e24c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3baf4b26d464f3ba0bb9f4bb94bc346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68f2c7f5f4f470f8e67530842bf8f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Load tokenizer and set pad_token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:46.305919Z",
     "iopub.status.busy": "2025-07-09T15:30:46.305699Z",
     "iopub.status.idle": "2025-07-09T15:30:46.309772Z",
     "shell.execute_reply": "2025-07-09T15:30:46.308882Z",
     "shell.execute_reply.started": "2025-07-09T15:30:46.305901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set pad_token (GPT-2 has no pad_token by default, so we use eos_token as pad_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:48.088869Z",
     "iopub.status.busy": "2025-07-09T15:30:48.088601Z",
     "iopub.status.idle": "2025-07-09T15:30:51.582011Z",
     "shell.execute_reply": "2025-07-09T15:30:51.581510Z",
     "shell.execute_reply.started": "2025-07-09T15:30:48.088849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798fa394923644ea876a1bc05c0a5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5256149c0ba94e52a8b1a7bff18f40fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Load model configuration and model\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")  # Load GPT-2 model configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", config=config) # Load GPT-2 causal language model with the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:30:55.934098Z",
     "iopub.status.busy": "2025-07-09T15:30:55.933527Z",
     "iopub.status.idle": "2025-07-09T15:30:55.958194Z",
     "shell.execute_reply": "2025-07-09T15:30:55.957526Z",
     "shell.execute_reply.started": "2025-07-09T15:30:55.934070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:31:08.821034Z",
     "iopub.status.busy": "2025-07-09T15:31:08.820760Z",
     "iopub.status.idle": "2025-07-09T15:31:11.692072Z",
     "shell.execute_reply": "2025-07-09T15:31:11.691555Z",
     "shell.execute_reply.started": "2025-07-09T15:31:08.821013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c4bb2ee574dd29f7f8be0515f8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcb8304f12b42f599d8dadcba88047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df658b45bb74429ca52889a1772c0f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f087ab5b36e14923a6af6abb8b61bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63194b1d8834efaa0b81dfd43b39bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Load SQuAD dataset and create train/validation splits\n",
    "dataset = load_dataset(\"squad\")\n",
    "vali_ds = vali_ds = dataset['validation'].select(range(5)) #select 5 examples for validation\n",
    "split_ds = split_ds = dataset['train'].train_test_split(test_size=0.1, seed=42) #train_test_split with test_size=0.1\n",
    "train_ds = split_ds['train'].shuffle(seed=42).select(range(2000)) #select 2000 shuffled examples\n",
    "eval_ds = split_ds['test'].shuffle(seed=42).select(range(200)) #select 200 shuffled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:31:15.731702Z",
     "iopub.status.busy": "2025-07-09T15:31:15.730948Z",
     "iopub.status.idle": "2025-07-09T15:31:15.735500Z",
     "shell.execute_reply": "2025-07-09T15:31:15.734925Z",
     "shell.execute_reply.started": "2025-07-09T15:31:15.731677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 2000\n",
      "Eval Dataset size: 200\n",
      "Validation Dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset size:\", len(train_ds))\n",
    "print(\"Eval Dataset size:\", len(eval_ds))\n",
    "print(\"Validation Dataset size:\", len(vali_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:31:16.837436Z",
     "iopub.status.busy": "2025-07-09T15:31:16.836860Z",
     "iopub.status.idle": "2025-07-09T15:31:16.843273Z",
     "shell.execute_reply": "2025-07-09T15:31:16.842636Z",
     "shell.execute_reply.started": "2025-07-09T15:31:16.837407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from Train Dataset: {'id': '5730eac6b7151e1900c015d4', 'title': 'Immaculate_Conception', 'context': 'On 28 February 1476, Pope Sixtus IV, authorized those dioceses that wished to introduce the feast to do so, and introduced it to his own diocese of Rome in 1477, with a specially composed Mass and Office of the feast. With his bull Cum praeexcelsa of 28 February 1477, in which he referred to the feast as that of the Conception of Mary, without using the word \"Immaculate\", he granted indulgences to those who would participate in the specially composed Mass or Office on the feast itself or during its octave, and he used the word \"immaculate\" of Mary, but applied instead the adjective \"miraculous\" to her conception. On 4 September 1483, referring to the feast as that of \"the Conception of Immaculate Mary ever Virgin\", he condemned both those who called it mortally sinful and heretical to hold that the \"glorious and immaculate mother of God was conceived without the stain of original sin\" and those who called it mortally sinful and heretical to hold that \"the glorious Virgin Mary was conceived with original sin\", since, he said, \"up to this time there has been no decision made by the Roman Church and the Apostolic See.\" This decree was reaffirmed by the Council of Trent.', 'question': 'What was the opposite of a sinner who committed a venial type? It would be the most grave type of all . ', 'answers': {'text': ['mortally sinful'], 'answer_start': [763]}}\n",
      "___________________________________________\n",
      "Example from Eval Dataset: {'id': '572eb6c0c246551400ce4547', 'title': 'Endangered_Species_Act', 'context': 'More than half of habitat for listed species is on non-federal property, owned by citizens, states, local governments, tribal governments and private organizations. Before the law was amended in 1982, a listed species could be taken only for scientific or research purposes. The amendment created a permit process to circumvent the take prohibition called a Habitat Conservation Plan or HCP to give incentives to non-federal land managers and private landowners to help protect listed and unlisted species, while allowing economic development that may harm (\"take\") the species.', 'question': 'What program gives incentives to private landowners to protect species on their land?', 'answers': {'text': ['Habitat Conservation Plan'], 'answer_start': [358]}}\n",
      "___________________________________________\n",
      "Example from Validation Dataset: {'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example from Train Dataset:\", train_ds[0])\n",
    "print(\"___________________________________________\")\n",
    "print(\"Example from Eval Dataset:\", eval_ds[0])\n",
    "print(\"___________________________________________\")\n",
    "print(\"Example from Validation Dataset:\", vali_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T09:23:56.678240Z",
     "iopub.status.busy": "2025-07-09T09:23:56.677928Z",
     "iopub.status.idle": "2025-07-09T09:23:56.682883Z",
     "shell.execute_reply": "2025-07-09T09:23:56.682134Z",
     "shell.execute_reply.started": "2025-07-09T09:23:56.678187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'context', 'question', 'answers']\n",
      "['id', 'title', 'context', 'question', 'answers']\n",
      "['id', 'title', 'context', 'question', 'answers']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.column_names)\n",
    "print(eval_ds.column_names)\n",
    "print(vali_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Free up memory by deleting large unused datasets \\\n",
    "#### Force garbage collection to immediately release memory resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:31:24.356692Z",
     "iopub.status.busy": "2025-07-09T15:31:24.356041Z",
     "iopub.status.idle": "2025-07-09T15:31:24.713682Z",
     "shell.execute_reply": "2025-07-09T15:31:24.712822Z",
     "shell.execute_reply.started": "2025-07-09T15:31:24.356667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear memory\n",
    "del dataset, split_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Handle edge cases (empty answers, truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:31:30.884654Z",
     "iopub.status.busy": "2025-07-09T15:31:30.883782Z",
     "iopub.status.idle": "2025-07-09T15:31:30.890775Z",
     "shell.execute_reply": "2025-07-09T15:31:30.890208Z",
     "shell.execute_reply.started": "2025-07-09T15:31:30.884627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Creates properly aligned input-label pairs for causal LM fine-tuning\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of SQuAD examples with 'context', 'question', 'answers'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'input_ids', 'attention_mask', 'labels'\n",
    "        \n",
    "    TODO: Implement the following steps:\n",
    "    1. For each example, create a prompt: \"Context: {context}\\nQuestion: {question}\\nAnswer: \"\n",
    "    2. Concatenate prompt + answer + eos_token\n",
    "    3. Tokenize the full sequence\n",
    "    4. Create labels by copying input_ids\n",
    "    5. Mask prompt tokens in labels with -100 (so loss only computed on answer)\n",
    "    \"\"\"\n",
    "    inputs = [] #Prompt + Answer + eos\n",
    "    #targets = []\n",
    "    ### Build prompt + answer for each example\n",
    "    for context, question, answers in zip(examples['context'], examples['question'], examples['answers']):\n",
    "        # TODO: Extract answer text (use first answer if multiple)\n",
    "        answer_text = answers['text'][0] if answers['text'] else ''\n",
    "        \n",
    "        # TODO: Create prompt and full sequence\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer: \"\n",
    "        full_text = prompt + answer_text + tokenizer.eos_token\n",
    "        \n",
    "        inputs.append(full_text)\n",
    "    \n",
    "    # TODO: Tokenize all sequences ##### no padding here, only truncation\n",
    "    model_inputs = tokenizer(inputs, padding=False, truncation=True, max_length=256, return_tensors=None)\n",
    "    \n",
    "    # TODO: Create labels with prompt masking\n",
    "    labels = []\n",
    "    for i, input_ids in enumerate(model_inputs['input_ids']):\n",
    "        # YOUR CODE HERE: Create label_ids, find prompt_length, mask prompt tokens\n",
    "                \n",
    "        # Get prompt text again to find its tokenized length\n",
    "        prompt_text = inputs[i].split(\"Answer:\")[0] + \"Answer: \"\n",
    "        \n",
    "        # Tokenize prompt to get length\n",
    "        prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "        prompt_length = len(prompt_ids)\n",
    "        \n",
    "        # Create label ids by copying input_ids\n",
    "        label_ids = input_ids.copy()\n",
    "        \n",
    "        # Mask prompt part: positions before prompt_length → -100\n",
    "        label_ids[:prompt_length] = [-100] * prompt_length\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Custom Data Collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching + Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:40:22.017797Z",
     "iopub.status.busy": "2025-07-09T15:40:22.017521Z",
     "iopub.status.idle": "2025-07-09T15:40:22.024653Z",
     "shell.execute_reply": "2025-07-09T15:40:22.024108Z",
     "shell.execute_reply.started": "2025-07-09T15:40:22.017774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QADataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for QA fine-tuning with proper padding\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        TODO: Implement proper batching with padding\n",
    "        1. Extract input_ids, attention_mask, labels from each feature\n",
    "        2. Pad sequences to max_length\n",
    "        3. Use pad_token_id for input_ids, 0 for attention_mask, -100 for labels\n",
    "        4. Return tensors\n",
    "        \"\"\"\n",
    "        # ✅ 1. Extract lists of input_ids, attention_mask, labels from features\n",
    "        input_ids = [f['input_ids'] for f in features]\n",
    "        attention_masks = [f['attention_mask'] for f in features]\n",
    "        labels = [f['labels'] for f in features]\n",
    "\n",
    "        # ✅ 2. Compute max_seq_length in this batch (capped at self.max_length)\n",
    "        batch_max_len = min(max(len(ids) for ids in input_ids), self.max_length)\n",
    "\n",
    "                # ✅ 3. Pad input_ids with tokenizer.pad_token_id\n",
    "        padded_input_ids = [ids[:batch_max_len] + [self.tokenizer.pad_token_id] * (batch_max_len - len(ids)) for ids in input_ids]\n",
    "\n",
    "        # ✅ 4. Pad attention_masks with 0\n",
    "        padded_attention_masks = [mask[:batch_max_len] + [0] * (batch_max_len - len(mask)) for mask in attention_masks]\n",
    "\n",
    "        # ✅ 5. Pad labels with -100 (for loss masking)\n",
    "        padded_labels = [lbls[:batch_max_len] + [-100] * (batch_max_len - len(lbls)) for lbls in labels]\n",
    "\n",
    "        # ✅ 6. Convert to tensors (efficient memory usage)\n",
    "        batch = {\n",
    "            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(padded_attention_masks, dtype=torch.long),\n",
    "            'labels': torch.tensor(padded_labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LoRA Con guration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:37:54.233852Z",
     "iopub.status.busy": "2025-07-09T16:37:54.233271Z",
     "iopub.status.idle": "2025-07-09T16:37:57.296480Z",
     "shell.execute_reply": "2025-07-09T16:37:57.295729Z",
     "shell.execute_reply.started": "2025-07-09T16:37:54.233830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:41:26.656582Z",
     "iopub.status.busy": "2025-07-09T16:41:26.656313Z",
     "iopub.status.idle": "2025-07-09T16:41:33.173779Z",
     "shell.execute_reply": "2025-07-09T16:41:33.173238Z",
     "shell.execute_reply.started": "2025-07-09T16:41:26.656557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhivaabolhadi\u001b[0m (\u001b[33mhivaabolhadizadeh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:52:13.584101Z",
     "iopub.status.busy": "2025-07-09T15:52:13.583759Z",
     "iopub.status.idle": "2025-07-09T15:52:15.400062Z",
     "shell.execute_reply": "2025-07-09T15:52:15.399479Z",
     "shell.execute_reply.started": "2025-07-09T15:52:13.584074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6088ec702cd34ae8b44b72d1187e35e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9867e8b0cb04013bf2311c326faa408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 2000\n",
      "Eval examples: 200\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply preprocessing to datasets\n",
    "tok_train_ds = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
    "tok_eval_ds = eval_ds.map(preprocess_function, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "print(f\"Training examples: {len(tok_train_ds)}\")\n",
    "print(f\"Eval examples: {len(tok_eval_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T15:53:08.775104Z",
     "iopub.status.busy": "2025-07-09T15:53:08.774417Z",
     "iopub.status.idle": "2025-07-09T15:53:09.240592Z",
     "shell.execute_reply": "2025-07-09T15:53:09.240038Z",
     "shell.execute_reply.started": "2025-07-09T15:53:08.775070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Setup LoRA configuration and model\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# TODO: Freeze base model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:00:06.361299Z",
     "iopub.status.busy": "2025-07-09T16:00:06.361077Z",
     "iopub.status.idle": "2025-07-09T16:00:06.365485Z",
     "shell.execute_reply": "2025-07-09T16:00:06.364740Z",
     "shell.execute_reply.started": "2025-07-09T16:00:06.361281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:42:41.560448Z",
     "iopub.status.busy": "2025-07-09T16:42:41.559772Z",
     "iopub.status.idle": "2025-07-09T16:42:41.564783Z",
     "shell.execute_reply": "2025-07-09T16:42:41.564087Z",
     "shell.execute_reply.started": "2025-07-09T16:42:41.560423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    # YOUR CODE HERE: Set r, lora_alpha, lora_dropout, target_modules, etc.\n",
    "    task_type=TaskType.CAUSAL_LM,           # Specifies the task type → Causal Language Modeling (Autoregressive generation)\n",
    "    r=8,                                    # Low-rank dimension for LoRA adapters → can experiment with 4, 16, or 32\n",
    "    lora_alpha=16,                          # Scaling factor for LoRA → typically set to 2 * r\n",
    "    lora_dropout=0.1,                       # Dropout rate for regularization to prevent overfitting\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],# Apply LoRA only to query and value projection layers → can add 'mlp' for wider adaptation\n",
    "    bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:16:11.574970Z",
     "iopub.status.busy": "2025-07-09T16:16:11.574706Z",
     "iopub.status.idle": "2025-07-09T16:16:11.601816Z",
     "shell.execute_reply": "2025-07-09T16:16:11.601097Z",
     "shell.execute_reply.started": "2025-07-09T16:16:11.574948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create LoRA model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:16:28.741999Z",
     "iopub.status.busy": "2025-07-09T16:16:28.741245Z",
     "iopub.status.idle": "2025-07-09T16:16:28.752467Z",
     "shell.execute_reply": "2025-07-09T16:16:28.751872Z",
     "shell.execute_reply.started": "2025-07-09T16:16:28.741974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify LoRA setup\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")\n",
    "\n",
    "lora_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:16:42.897612Z",
     "iopub.status.busy": "2025-07-09T16:16:42.896776Z",
     "iopub.status.idle": "2025-07-09T16:16:42.901320Z",
     "shell.execute_reply": "2025-07-09T16:16:42.900595Z",
     "shell.execute_reply.started": "2025-07-09T16:16:42.897577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create data collator instance\n",
    "data_collator = QADataCollator(tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:47:57.491354Z",
     "iopub.status.busy": "2025-07-09T16:47:57.491041Z",
     "iopub.status.idle": "2025-07-09T16:48:04.882261Z",
     "shell.execute_reply": "2025-07-09T16:48:04.881679Z",
     "shell.execute_reply.started": "2025-07-09T16:47:57.491333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250709_164757-yor1lfpv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv' target=\"_blank\">r8_lr2e-4_attn</a></strong> to <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79127d45a410>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# W&B\n",
    "wandb.init(project=\"lora_squad_finetuning\", name=\"r8_lr2e-4_attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:54:41.271256Z",
     "iopub.status.busy": "2025-07-09T16:54:41.270940Z",
     "iopub.status.idle": "2025-07-09T16:54:41.276895Z",
     "shell.execute_reply": "2025-07-09T16:54:41.276199Z",
     "shell.execute_reply.started": "2025-07-09T16:54:41.271235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"rank\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"lora_dropout\": lora_config.lora_dropout,\n",
    "    \"target_modules\": lora_config.target_modules\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:43:30.160847Z",
     "iopub.status.busy": "2025-07-09T16:43:30.160585Z",
     "iopub.status.idle": "2025-07-09T16:43:30.164539Z",
     "shell.execute_reply": "2025-07-09T16:43:30.163803Z",
     "shell.execute_reply.started": "2025-07-09T16:43:30.160827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:55:13.992550Z",
     "iopub.status.busy": "2025-07-09T16:55:13.991994Z",
     "iopub.status.idle": "2025-07-09T16:55:14.025775Z",
     "shell.execute_reply": "2025-07-09T16:55:14.025093Z",
     "shell.execute_reply.started": "2025-07-09T16:55:13.992524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_squad_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\",  #  W&B\n",
    "    fp16=True,  # GPU\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=2,  #(pitfall 2: Memory Leaks)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:56:09.483064Z",
     "iopub.status.busy": "2025-07-09T16:56:09.482522Z",
     "iopub.status.idle": "2025-07-09T16:56:09.488411Z",
     "shell.execute_reply": "2025-07-09T16:56:09.487875Z",
     "shell.execute_reply.started": "2025-07-09T16:56:09.483022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:57:37.062166Z",
     "iopub.status.busy": "2025-07-09T16:57:37.061826Z",
     "iopub.status.idle": "2025-07-09T16:57:38.891489Z",
     "shell.execute_reply": "2025-07-09T16:57:38.890738Z",
     "shell.execute_reply.started": "2025-07-09T16:57:37.062142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T16:57:40.786985Z",
     "iopub.status.busy": "2025-07-09T16:57:40.786273Z",
     "iopub.status.idle": "2025-07-09T16:57:41.310698Z",
     "shell.execute_reply": "2025-07-09T16:57:41.309972Z",
     "shell.execute_reply.started": "2025-07-09T16:57:40.786951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create trainer\n",
    "trainer = Trainer(\n",
    "    # YOUR CODE HERE: Set model, args, datasets, data_collator\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_train_ds,\n",
    "    eval_dataset=tok_eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:00:23.785321Z",
     "iopub.status.busy": "2025-07-09T17:00:23.784703Z",
     "iopub.status.idle": "2025-07-09T17:03:30.553814Z",
     "shell.execute_reply": "2025-07-09T17:03:30.553229Z",
     "shell.execute_reply.started": "2025-07-09T17:00:23.785296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 03:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.310377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.309063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.3189207914645079, metrics={'train_runtime': 186.1897, 'train_samples_per_second': 32.225, 'train_steps_per_second': 1.015, 'total_flos': 781013331836928.0, 'train_loss': 0.3189207914645079, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:05:38.555419Z",
     "iopub.status.busy": "2025-07-09T17:05:38.554665Z",
     "iopub.status.idle": "2025-07-09T17:05:42.476568Z",
     "shell.execute_reply": "2025-07-09T17:05:42.475940Z",
     "shell.execute_reply.started": "2025-07-09T17:05:38.555393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.3090629279613495, 'eval_runtime': 3.9098, 'eval_samples_per_second': 51.153, 'eval_steps_per_second': 3.325, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate the model\n",
    "evaluation_result = trainer.evaluate()\n",
    "print(\"Evaluation results:\", evaluation_result)\n",
    "wandb.log({\"eval_loss\": evaluation_result[\"eval_loss\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:14:26.602594Z",
     "iopub.status.busy": "2025-07-09T17:14:26.601671Z",
     "iopub.status.idle": "2025-07-09T17:14:26.844390Z",
     "shell.execute_reply": "2025-07-09T17:14:26.843806Z",
     "shell.execute_reply.started": "2025-07-09T17:14:26.602557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 5 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/wandb/run-20250709_164757-yor1lfpv/files/lora_squad_output/README.md',\n",
       " '/kaggle/working/wandb/run-20250709_164757-yor1lfpv/files/lora_squad_output/adapter_config.json',\n",
       " '/kaggle/working/wandb/run-20250709_164757-yor1lfpv/files/lora_squad_output/checkpoint-126',\n",
       " '/kaggle/working/wandb/run-20250709_164757-yor1lfpv/files/lora_squad_output/adapter_model.safetensors',\n",
       " '/kaggle/working/wandb/run-20250709_164757-yor1lfpv/files/lora_squad_output/checkpoint-189']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Save the fine-tuned model\n",
    "lora_model.save_pretrained(\"./lora_squad_output\")\n",
    "wandb.save(\"./lora_squad_output/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:18:04.995627Z",
     "iopub.status.busy": "2025-07-09T17:18:04.995037Z",
     "iopub.status.idle": "2025-07-09T17:18:05.001585Z",
     "shell.execute_reply": "2025-07-09T17:18:05.000862Z",
     "shell.execute_reply.started": "2025-07-09T17:18:04.995604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in lora_squad_output: ['adapter_config.json', 'adapter_model.safetensors', 'checkpoint-189', 'checkpoint-126', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Files in lora_squad_output:\", os.listdir(\"./lora_squad_output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Inference and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:06.707224Z",
     "iopub.status.busy": "2025-07-09T17:22:06.706647Z",
     "iopub.status.idle": "2025-07-09T17:22:06.714342Z",
     "shell.execute_reply": "2025-07-09T17:22:06.713750Z",
     "shell.execute_reply.started": "2025-07-09T17:22:06.707200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, context, question, max_new_tokens=50, decoding_strategy=\"greedy\", **kwargs):\n",
    "    \"\"\"\n",
    "    Generate answer for a given context and question\n",
    "    \n",
    "    TODO: Implement inference function\n",
    "    1. Create prompt\n",
    "    2. Tokenize input\n",
    "    3. Generate with appropriate sampling parameters\n",
    "    4. Decode and extract only the generated answer part\n",
    "    \"\"\"\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    \n",
    "    if decoding_strategy == \"greedy\":\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    elif decoding_strategy == \"top_k\":\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=kwargs.get(\"top_k\", 50),\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    elif decoding_strategy == \"nucleus\":\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=kwargs.get(\"top_p\", 0.9),\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    elif decoding_strategy == \"temperature\":\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=kwargs.get(\"temperature\", 1.0),\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = decoded_output.split(\"Answer: \")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:20:42.919092Z",
     "iopub.status.busy": "2025-07-09T17:20:42.918580Z",
     "iopub.status.idle": "2025-07-09T17:20:43.459837Z",
     "shell.execute_reply": "2025-07-09T17:20:43.459267Z",
     "shell.execute_reply.started": "2025-07-09T17:20:42.919070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load fine-tuned model for testing\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\"./lora_squad_output\")\n",
    "ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:11.104824Z",
     "iopub.status.busy": "2025-07-09T17:22:11.104331Z",
     "iopub.status.idle": "2025-07-09T17:22:12.511584Z",
     "shell.execute_reply": "2025-07-09T17:22:12.510806Z",
     "shell.execute_reply.started": "2025-07-09T17:22:11.104799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Fine-tuned Model ===\n",
      "\n",
      "Example 1:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Predicted Answer: The Denver Broncos.\n",
      "Ground Truth: Denver Broncos\n",
      "\n",
      "Example 2:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Predicted Answer: The Denver Broncos.\n",
      "Ground Truth: Carolina Panthers\n",
      "\n",
      "Example 3:\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "Question: Where did Super Bowl 50 take place?\n",
      "Predicted Answer: The Super Bowl 50 was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold\n",
      "Ground Truth: Santa Clara, California\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test on validation examples\n",
    "print(\"\\n=== Testing Fine-tuned Model ===\")\n",
    "for i, example in enumerate(vali_ds):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    ground_truth = example['answers']['text'][0] if example['answers']['text'] else ''\n",
    "    predicted_answer = generate_answer(ft_model, tokenizer, context, question, decoding_strategy=\"greedy\")\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Context: {context[:100]}...\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:42:07.911401Z",
     "iopub.status.busy": "2025-07-09T17:42:07.911086Z",
     "iopub.status.idle": "2025-07-09T17:42:11.187766Z",
     "shell.execute_reply": "2025-07-09T17:42:11.187093Z",
     "shell.execute_reply.started": "2025-07-09T17:42:07.911378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:42:13.594549Z",
     "iopub.status.busy": "2025-07-09T17:42:13.593665Z",
     "iopub.status.idle": "2025-07-09T17:42:13.599928Z",
     "shell.execute_reply": "2025-07-09T17:42:13.599241Z",
     "shell.execute_reply.started": "2025-07-09T17:42:13.594508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compute SQuAD metrics\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:42:36.434475Z",
     "iopub.status.busy": "2025-07-09T17:42:36.433529Z",
     "iopub.status.idle": "2025-07-09T17:42:38.767177Z",
     "shell.execute_reply": "2025-07-09T17:42:38.766437Z",
     "shell.execute_reply.started": "2025-07-09T17:42:36.434440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb949aba12d4fde9475ffc885357903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f8305cb90f4b2c9a9461507602238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQuAD Metrics: {'exact_match': 40.0, 'f1': 49.77777777777778}\n"
     ]
    }
   ],
   "source": [
    "for example in vali_ds:\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    ground_truth = example['answers']['text'][0] if example['answers']['text'] else ''\n",
    "    predicted_answer = generate_answer(ft_model, tokenizer, context, question, decoding_strategy=\"greedy\")\n",
    "    predictions.append({\"id\": example['id'], \"prediction_text\": predicted_answer})\n",
    "    references.append({\"id\": example['id'], \"answers\": example['answers']})\n",
    "\n",
    "try:\n",
    "    squad_metric = evaluate.load(\"squad\")  \n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\nSQuAD Metrics: {results}\")\n",
    "    wandb.log({\"squad_exact_match\": results[\"exact_match\"], \"squad_f1\": results[\"f1\"]})\n",
    "except Exception as e:\n",
    "    print(f\"Could not load SQuAD metric: {e}\")\n",
    "    print(\"Install with: pip install evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:44:26.168271Z",
     "iopub.status.busy": "2025-07-09T17:44:26.167487Z",
     "iopub.status.idle": "2025-07-09T17:44:29.029655Z",
     "shell.execute_reply": "2025-07-09T17:44:29.029025Z",
     "shell.execute_reply.started": "2025-07-09T17:44:26.168247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Baseline Model ===\n",
      "\n",
      "Baseline SQuAD Metrics: {'exact_match': 0.0, 'f1': 16.984206664864654}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Testing Baseline Model ===\")\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "baseline_predictions = []\n",
    "for example in vali_ds:\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    predicted_answer = generate_answer(baseline_model, tokenizer, context, question, decoding_strategy=\"greedy\")\n",
    "    baseline_predictions.append({\"id\": example['id'], \"prediction_text\": predicted_answer})\n",
    "\n",
    "try:\n",
    "    baseline_results = squad_metric.compute(predictions=baseline_predictions, references=references)\n",
    "    print(f\"\\nBaseline SQuAD Metrics: {baseline_results}\")\n",
    "    wandb.log({\"baseline_exact_match\": baseline_results[\"exact_match\"], \"baseline_f1\": baseline_results[\"f1\"]})\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute baseline SQuAD metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:47:19.427142Z",
     "iopub.status.busy": "2025-07-09T17:47:19.426609Z",
     "iopub.status.idle": "2025-07-09T17:47:19.444463Z",
     "shell.execute_reply": "2025-07-09T17:47:19.443876Z",
     "shell.execute_reply.started": "2025-07-09T17:47:19.427117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Failure Case Analysis ===\n",
      "\n",
      "Failure Case 1:\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Predicted: The Denver Broncos.\n",
      "Ground Truth: Denver Broncos\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "\n",
      "Failure Case 2:\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Predicted: The Denver Broncos.\n",
      "Ground Truth: Carolina Panthers\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "\n",
      "Failure Case 3:\n",
      "Question: Where did Super Bowl 50 take place?\n",
      "Predicted: The Super Bowl 50 was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold\n",
      "Ground Truth: Santa Clara, California\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "\n",
      "Failure Case 4:\n",
      "Question: Which NFL team won Super Bowl 50?\n",
      "Predicted: The Denver Broncos.\n",
      "Ground Truth: Denver Broncos\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n",
      "\n",
      "Failure Case 5:\n",
      "Question: What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
      "Predicted: The blue.\n",
      "Question: What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
      "Answer\n",
      "Ground Truth: gold\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football Leagu...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Failure Case Analysis ===\")\n",
    "for i, (pred, ref) in enumerate(zip(predictions, references)):\n",
    "    if pred[\"prediction_text\"] != ref[\"answers\"][\"text\"][0]:\n",
    "        print(f\"\\nFailure Case {i+1}:\")\n",
    "        print(f\"Question: {vali_ds[i]['question']}\")\n",
    "        print(f\"Predicted: {pred['prediction_text']}\")\n",
    "        print(f\"Ground Truth: {ref['answers']['text'][0]}\")\n",
    "        print(f\"Context: {vali_ds[i]['context'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generation Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:48:23.477205Z",
     "iopub.status.busy": "2025-07-09T17:48:23.476483Z",
     "iopub.status.idle": "2025-07-09T17:48:23.482629Z",
     "shell.execute_reply": "2025-07-09T17:48:23.481892Z",
     "shell.execute_reply.started": "2025-07-09T17:48:23.477179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decoding_strategies = [\n",
    "    {\"strategy\": \"greedy\"},\n",
    "    {\"strategy\": \"top_k\", \"top_k\": 10},\n",
    "    {\"strategy\": \"top_k\", \"top_k\": 25},\n",
    "    {\"strategy\": \"top_k\", \"top_k\": 50},\n",
    "    {\"strategy\": \"nucleus\", \"top_p\": 0.8},\n",
    "    {\"strategy\": \"nucleus\", \"top_p\": 0.9},\n",
    "    {\"strategy\": \"nucleus\", \"top_p\": 0.95},\n",
    "    {\"strategy\": \"temperature\", \"temperature\": 0.7},\n",
    "    {\"strategy\": \"temperature\", \"temperature\": 1.0},\n",
    "    {\"strategy\": \"temperature\", \"temperature\": 1.3},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:48:41.401667Z",
     "iopub.status.busy": "2025-07-09T17:48:41.401125Z",
     "iopub.status.idle": "2025-07-09T17:49:03.489328Z",
     "shell.execute_reply": "2025-07-09T17:49:03.488546Z",
     "shell.execute_reply.started": "2025-07-09T17:48:41.401645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decoding Strategy Analysis ===\n",
      "\n",
      "Strategy: greedy {}\n",
      "SQuAD Metrics: {'exact_match': 40.0, 'f1': 49.77777777777778}\n",
      "\n",
      "Strategy: top_k {'top_k': 10}\n",
      "SQuAD Metrics: {'exact_match': 20.0, 'f1': 28.353658536585368}\n",
      "\n",
      "Strategy: top_k {'top_k': 25}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 8.791208791208792}\n",
      "\n",
      "Strategy: top_k {'top_k': 50}\n",
      "SQuAD Metrics: {'exact_match': 20.0, 'f1': 20.0}\n",
      "\n",
      "Strategy: nucleus {'top_p': 0.8}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 8.148148148148149}\n",
      "\n",
      "Strategy: nucleus {'top_p': 0.9}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 4.651162790697674}\n",
      "\n",
      "Strategy: nucleus {'top_p': 0.95}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 0.9523809523809523}\n",
      "\n",
      "Strategy: temperature {'temperature': 0.7}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 24.000000000000004}\n",
      "\n",
      "Strategy: temperature {'temperature': 1.0}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 12.352941176470589}\n",
      "\n",
      "Strategy: temperature {'temperature': 1.3}\n",
      "SQuAD Metrics: {'exact_match': 0.0, 'f1': 6.128048780487805}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Decoding Strategy Analysis ===\")\n",
    "for strategy in decoding_strategies:\n",
    "    strategy_name = strategy[\"strategy\"]\n",
    "    strategy_params = {k: v for k, v in strategy.items() if k != \"strategy\"}\n",
    "    predictions = []\n",
    "    for example in vali_ds:\n",
    "        context = example['context']\n",
    "        question = example['question']\n",
    "        predicted_answer = generate_answer(ft_model, tokenizer, context, question, \n",
    "                                         decoding_strategy=strategy_name, **strategy_params)\n",
    "        predictions.append({\"id\": example['id'], \"prediction_text\": predicted_answer})\n",
    "    \n",
    "    try:\n",
    "        results = squad_metric.compute(predictions=predictions, references=references)\n",
    "        print(f\"\\nStrategy: {strategy_name} {strategy_params}\")\n",
    "        print(f\"SQuAD Metrics: {results}\")\n",
    "        wandb.log({f\"{strategy_name}_{str(strategy_params)}_exact_match\": results[\"exact_match\"],\n",
    "                   f\"{strategy_name}_{str(strategy_params)}_f1\": results[\"f1\"]})\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute SQuAD metrics for {strategy_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:03:56.817691Z",
     "iopub.status.busy": "2025-07-09T18:03:56.817395Z",
     "iopub.status.idle": "2025-07-09T18:03:56.836580Z",
     "shell.execute_reply": "2025-07-09T18:03:56.836004Z",
     "shell.execute_reply.started": "2025-07-09T18:03:56.817671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(ranks=[4, 8, 16, 32, 64], lrs=[7e-5, 1e-4, 2e-4, 5e-4, 1e-3], \n",
    "                  target_modules=[[\"attn.c_attn\", \"attn.c_proj\"], \n",
    "                                  [\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n",
    "                                  [\"q_proj\", \"v_proj\", \"attn.c_attn\", \"attn.c_proj\"]]):\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Check the number of validation samples\n",
    "    print(f\"Number of validation samples: {len(vali_ds)}\")\n",
    "    \n",
    "    for r in ranks:\n",
    "        for lr in lrs:\n",
    "            for modules in target_modules:\n",
    "                # Define unique run name for W&B\n",
    "                run_name = f\"r{r}_lr{lr}_{'_'.join(modules)}\"\n",
    "                print(f\"\\nRunning experiment: {run_name}\")\n",
    "                \n",
    "                # Initialize W&B run\n",
    "                wandb.init(project=\"lora_squad_finetuning\", name=run_name)\n",
    "                \n",
    "                # Reset and load base model\n",
    "                model = AutoModelForCausalLM.from_pretrained(\"gpt2\", config=config).to(device)\n",
    "                # Freeze all parameters to apply LoRA\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "                # Configure LoRA\n",
    "                lora_config = LoraConfig(\n",
    "                    task_type=TaskType.CAUSAL_LM,\n",
    "                    r=r,\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    target_modules=modules,\n",
    "                    bias=\"none\"\n",
    "                )\n",
    "                lora_model = get_peft_model(model, lora_config)\n",
    "                \n",
    "                # Log number of trainable parameters\n",
    "                trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "                wandb.log({\"trainable_params\": trainable_params})\n",
    "                \n",
    "                # Set up TrainingArguments\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=f\"./lora_{run_name}\",\n",
    "                    eval_strategy=\"epoch\",\n",
    "                    save_strategy=\"epoch\",\n",
    "                    learning_rate=lr,\n",
    "                    per_device_train_batch_size=8,\n",
    "                    per_device_eval_batch_size=8,\n",
    "                    num_train_epochs=5,  # Increased epochs for better training\n",
    "                    weight_decay=0.01,\n",
    "                    logging_dir=f\"./logs_{run_name}\",\n",
    "                    logging_steps=100,\n",
    "                    report_to=\"wandb\",\n",
    "                    fp16=True,  # Enable mixed precision for efficiency\n",
    "                    load_best_model_at_end=True,\n",
    "                    metric_for_best_model=\"eval_loss\",\n",
    "                    save_total_limit=2,\n",
    "                    gradient_accumulation_steps=4,  # Increased for stable gradients\n",
    "                )\n",
    "                \n",
    "                # Log hyperparameters to W&B\n",
    "                wandb.config.update({\n",
    "                    \"rank\": r,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"target_modules\": modules,\n",
    "                    \"lora_alpha\": 16,\n",
    "                    \"lora_dropout\": 0.1,\n",
    "                    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "                    \"num_epochs\": training_args.num_train_epochs\n",
    "                })\n",
    "                \n",
    "                # Create Trainer instance\n",
    "                trainer = Trainer(\n",
    "                    model=lora_model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tok_train_ds,\n",
    "                    eval_dataset=tok_eval_ds,\n",
    "                    data_collator=data_collator,\n",
    "                )\n",
    "                \n",
    "                # Train and evaluate the model\n",
    "                trainer.train()\n",
    "                eval_result = trainer.evaluate()\n",
    "                \n",
    "                # Log training and evaluation loss\n",
    "                train_loss = trainer.state.log_history[-2].get(\"loss\", float('inf'))\n",
    "                eval_loss = eval_result[\"eval_loss\"]\n",
    "                wandb.log({\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"eval_loss\": eval_loss,\n",
    "                    \"training_steps\": trainer.state.global_step,\n",
    "                    \"training_time\": trainer.state.log_history[-1].get(\"train_runtime\", 0)\n",
    "                })\n",
    "                \n",
    "                # Compute SQuAD metrics for Greedy and Top-k strategies\n",
    "                for strategy in [{\"strategy\": \"greedy\"}, {\"strategy\": \"top_k\", \"top_k\": 5}]:\n",
    "                    strategy_name = strategy[\"strategy\"]\n",
    "                    strategy_params = {k: v for k, v in strategy.items() if k != \"strategy\"}\n",
    "                    predictions = []\n",
    "                    for example in vali_ds:\n",
    "                        context = example['context']\n",
    "                        question = example['question']\n",
    "                        predicted_answer = generate_answer(lora_model, tokenizer, context, question, \n",
    "                                                         decoding_strategy=strategy_name, **strategy_params)\n",
    "                        predictions.append({\"id\": example['id'], \"prediction_text\": predicted_answer})\n",
    "                    \n",
    "                    try:\n",
    "                        squad_metric = evaluate.load(\"squad\")\n",
    "                        squad_results = squad_metric.compute(predictions=predictions, references=references)\n",
    "                        wandb.log({\n",
    "                            f\"squad_exact_match_{strategy_name}_{str(strategy_params)}\": squad_results[\"exact_match\"],\n",
    "                            f\"squad_f1_{strategy_name}_{str(strategy_params)}\": squad_results[\"f1\"]\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not compute SQuAD metrics for {strategy_name}: {e}\")\n",
    "                \n",
    "                # Store results for this experiment\n",
    "                results.append({\n",
    "                    \"rank\": r,\n",
    "                    \"lr\": lr,\n",
    "                    \"modules\": modules,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"eval_loss\": eval_loss,\n",
    "                    \"trainable_params\": trainable_params,\n",
    "                    \"squad_exact_match\": squad_results.get(\"exact_match\", 0),\n",
    "                    \"squad_f1\": squad_results.get(\"f1\", 0)\n",
    "                })\n",
    "                \n",
    "                # Save model and upload to W&B\n",
    "                lora_model.save_pretrained(f\"./lora_{run_name}\")\n",
    "                wandb.save(f\"./lora_{run_name}/*\")\n",
    "                \n",
    "                # Free up memory (pitfall 2: Memory Leaks)\n",
    "                lora_model.cpu()\n",
    "                del lora_model, trainer, model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Finish W&B run\n",
    "                wandb.finish()\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for result in results:\n",
    "        plt.plot([1, 2, 3, 4, 5], [result[\"train_loss\"]] * 5, label=f\"Train r={result['rank']}, lr={result['lr']}, {result['modules']}\")\n",
    "        plt.plot([1, 2, 3, 4, 5], [result[\"eval_loss\"]] * 5, '--', label=f\"Eval r={result['rank']}, lr={result['lr']}, {result['modules']}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    wandb.log({\"loss_plot\": wandb.Image(plt)})\n",
    "    \n",
    "    # Plot parameter count vs. performance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for result in results:\n",
    "        plt.scatter(result[\"trainable_params\"], result[\"eval_loss\"], label=f\"r={result['rank']}, lr={result['lr']}, {result['modules']}\")\n",
    "        plt.scatter(result[\"trainable_params\"], result[\"squad_f1\"], marker='x', label=f\"F1 r={result['rank']}, lr={result['lr']}, {result['modules']}\")\n",
    "    plt.xlabel(\"Number of Trainable Parameters\")\n",
    "    plt.ylabel(\"Evaluation Loss / SQuAD F1\")\n",
    "    plt.title(\"Parameter Count vs. Performance\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    wandb.log({\"param_vs_performance_plot\": wandb.Image(plt)})\n",
    "    \n",
    "    # Print computational cost analysis\n",
    "    print(\"\\nComputational Cost Analysis:\")\n",
    "    for result in results:\n",
    "        print(f\"r={result['rank']}, lr={result['lr']}, modules={result['modules']}: \"\n",
    "              f\"Eval Loss={result['eval_loss']:.4f}, Params={result['trainable_params']}, \"\n",
    "              f\"SQuAD F1={result['squad_f1']:.4f}, Exact Match={result['squad_exact_match']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:04:13.305636Z",
     "iopub.status.busy": "2025-07-09T18:04:13.305094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 5\n",
      "\n",
      "Running experiment: r4_lr7e-05_attn.c_attn_attn.c_proj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>baseline_exact_match</td><td>▁</td></tr><tr><td>baseline_f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▁█▁</td></tr><tr><td>eval/samples_per_second</td><td>▂█▁█</td></tr><tr><td>eval/steps_per_second</td><td>▂█▁█</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>greedy_{}_exact_match</td><td>▁</td></tr><tr><td>greedy_{}_f1</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.8}_exact_match</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.8}_f1</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.95}_exact_match</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.95}_f1</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.9}_exact_match</td><td>▁</td></tr><tr><td>nucleus_{'top_p': 0.9}_f1</td><td>▁</td></tr><tr><td>squad_exact_match</td><td>▁</td></tr><tr><td>squad_f1</td><td>▁</td></tr><tr><td>temperature_{'temperature': 0.7}_exact_match</td><td>▁</td></tr><tr><td>temperature_{'temperature': 0.7}_f1</td><td>▁</td></tr><tr><td>temperature_{'temperature': 1.0}_exact_match</td><td>▁</td></tr><tr><td>temperature_{'temperature': 1.0}_f1</td><td>▁</td></tr><tr><td>temperature_{'temperature': 1.3}_exact_match</td><td>▁</td></tr><tr><td>temperature_{'temperature': 1.3}_f1</td><td>▁</td></tr><tr><td>top_k_{'top_k': 10}_exact_match</td><td>▁</td></tr><tr><td>top_k_{'top_k': 10}_f1</td><td>▁</td></tr><tr><td>top_k_{'top_k': 25}_exact_match</td><td>▁</td></tr><tr><td>top_k_{'top_k': 25}_f1</td><td>▁</td></tr><tr><td>top_k_{'top_k': 50}_exact_match</td><td>▁</td></tr><tr><td>top_k_{'top_k': 50}_f1</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▃▅███</td></tr><tr><td>train/global_step</td><td>▁▃▅████████████████</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_exact_match</td><td>0</td></tr><tr><td>baseline_f1</td><td>16.98421</td></tr><tr><td>eval/loss</td><td>0.30906</td></tr><tr><td>eval/runtime</td><td>3.9098</td></tr><tr><td>eval/samples_per_second</td><td>51.153</td></tr><tr><td>eval/steps_per_second</td><td>3.325</td></tr><tr><td>eval_loss</td><td>0.30906</td></tr><tr><td>greedy_{}_exact_match</td><td>40</td></tr><tr><td>greedy_{}_f1</td><td>49.77778</td></tr><tr><td>nucleus_{'top_p': 0.8}_exact_match</td><td>0</td></tr><tr><td>nucleus_{'top_p': 0.8}_f1</td><td>8.14815</td></tr><tr><td>nucleus_{'top_p': 0.95}_exact_match</td><td>0</td></tr><tr><td>nucleus_{'top_p': 0.95}_f1</td><td>0.95238</td></tr><tr><td>nucleus_{'top_p': 0.9}_exact_match</td><td>0</td></tr><tr><td>nucleus_{'top_p': 0.9}_f1</td><td>4.65116</td></tr><tr><td>squad_exact_match</td><td>40</td></tr><tr><td>squad_f1</td><td>49.77778</td></tr><tr><td>temperature_{'temperature': 0.7}_exact_match</td><td>0</td></tr><tr><td>temperature_{'temperature': 0.7}_f1</td><td>24.0</td></tr><tr><td>temperature_{'temperature': 1.0}_exact_match</td><td>0</td></tr><tr><td>temperature_{'temperature': 1.0}_f1</td><td>12.35294</td></tr><tr><td>temperature_{'temperature': 1.3}_exact_match</td><td>0</td></tr><tr><td>temperature_{'temperature': 1.3}_f1</td><td>6.12805</td></tr><tr><td>top_k_{'top_k': 10}_exact_match</td><td>20</td></tr><tr><td>top_k_{'top_k': 10}_f1</td><td>28.35366</td></tr><tr><td>top_k_{'top_k': 25}_exact_match</td><td>0</td></tr><tr><td>top_k_{'top_k': 25}_f1</td><td>8.79121</td></tr><tr><td>top_k_{'top_k': 50}_exact_match</td><td>20</td></tr><tr><td>top_k_{'top_k': 50}_f1</td><td>20</td></tr><tr><td>total_flos</td><td>781013331836928.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>189</td></tr><tr><td>train/grad_norm</td><td>142347.04688</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.416</td></tr><tr><td>train_loss</td><td>0.31892</td></tr><tr><td>train_runtime</td><td>186.1897</td></tr><tr><td>train_samples_per_second</td><td>32.225</td></tr><tr><td>train_steps_per_second</td><td>1.015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">r8_lr2e-4_attn</strong> at: <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/yor1lfpv</a><br> View project at: <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250709_164757-yor1lfpv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250709_180413-9iild8aj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/9iild8aj' target=\"_blank\">r4_lr7e-05_attn.c_attn_attn.c_proj</a></strong> to <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/9iild8aj' target=\"_blank\">https://wandb.ai/hivaabolhadizadeh/lora_squad_finetuning/runs/9iild8aj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 85/160 02:31 < 02:16, 0.55 it/s, Epoch 2.64/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.306402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.258926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the experiment\n",
    "experiment_results = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
